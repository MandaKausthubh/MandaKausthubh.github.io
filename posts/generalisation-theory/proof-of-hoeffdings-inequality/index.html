<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=content-type content="text/html"><meta name=viewport content="width=device-width,initial-scale=1"><title itemprop=name>Hoeffding's Inequality and Generalization | Manda Kausthubh's blog</title>
<meta property="og:title" content="Hoeffding's Inequality and Generalization | Manda Kausthubh's blog"><meta name=twitter:title content="Hoeffding's Inequality and Generalization | Manda Kausthubh's blog"><meta itemprop=name content="Hoeffding's Inequality and Generalization | Manda Kausthubh's blog"><meta name=application-name content="Hoeffding's Inequality and Generalization | Manda Kausthubh's blog"><meta property="og:site_name" content="Awesome hugo blog"><meta name=description content="Minimal Hugo blog theme with light and dark mode support"><meta itemprop=description content="Minimal Hugo blog theme with light and dark mode support"><meta property="og:description" content="Minimal Hugo blog theme with light and dark mode support"><meta name=twitter:description content="Minimal Hugo blog theme with light and dark mode support"><meta property="og:locale" content="en"><meta name=language content="en"><link rel=alternate hreflang=en href=https://mandakausthubh.github.io/posts/generalisation-theory/proof-of-hoeffdings-inequality/ title=English><meta property="og:type" content="article"><meta property="og:article:published_time" content=2025-04-13T03:53:13+0530><meta property="article:published_time" content=2025-04-13T03:53:13+0530><meta property="og:url" content="https://mandakausthubh.github.io/posts/generalisation-theory/proof-of-hoeffdings-inequality/"><meta property="og:article:author" content="Kausthubh Manda"><meta property="article:author" content="Kausthubh Manda"><meta name=author content="Kausthubh Manda"><script defer type=application/ld+json>{"@context":"http://schema.org","@type":"Article","headline":"Hoeffding's Inequality and Generalization","author":{"@type":"Person","name":""},"datePublished":"2025-04-13","description":"","wordCount":927,"mainEntityOfPage":"True","dateModified":"2025-04-13","image":{"@type":"imageObject","url":""},"publisher":{"@type":"Organization","name":"Manda Kausthubh\u0027s blog"}}</script><meta name=generator content="Hugo 0.146.2"><meta property="og:url" content="https://mandakausthubh.github.io/posts/generalisation-theory/proof-of-hoeffdings-inequality/"><meta property="og:site_name" content="Manda Kausthubh's blog"><meta property="og:title" content="Hoeffding's Inequality and Generalization"><meta property="og:description" content="What is Hoeffding’s Inequality ? Given a sequence of Random Variables that act as I.I.Ds (Identical Independently Distributions) $Z_{1}, Z_{2}, Z_{3} \dots Z_{n}$ with mean $\mu$ and $\mathbf{P}(a \leq Z_{i} \leq b) = 1$ then for any $\epsilon > 0$ we have the following:
$$ \mathbf{P}(|\bar{Z}_{n} - \mu| > \epsilon) \leq \exp\left( \frac{-2n\epsilon^{2}}{(b-a)^{2}} \right) $$ where $\bar{Z_{n}}=\frac{1}{n} \sum_{i=1}^{n}Z_{i}$.
Why should anyone care about this at all?? This is well explained in Abu Mostafa’s book: Learning from data: A Short Course. This is an excellent book for anyone to get started on theoretical machine learning and is a must read. This book was based on the course that he had taught at CalTech. Although this book doesn’t go into the actual proof of Hoeffding’s in-equality this blog aims of explaining Hoeffding’s inequality in a rigorous manner while also providing great intuition. Most of this blog is originally taken, rather blatantly, from Stanford CS229: Supplemental Lecture notes: Hoeffding’s inequality."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-13T03:53:13+05:30"><meta property="article:modified_time" content="2025-04-13T03:53:13+05:30"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hoeffding's Inequality and Generalization"><meta name=twitter:description content="What is Hoeffding’s Inequality ? Given a sequence of Random Variables that act as I.I.Ds (Identical Independently Distributions) $Z_{1}, Z_{2}, Z_{3} \dots Z_{n}$ with mean $\mu$ and $\mathbf{P}(a \leq Z_{i} \leq b) = 1$ then for any $\epsilon > 0$ we have the following:
$$ \mathbf{P}(|\bar{Z}_{n} - \mu| > \epsilon) \leq \exp\left( \frac{-2n\epsilon^{2}}{(b-a)^{2}} \right) $$ where $\bar{Z_{n}}=\frac{1}{n} \sum_{i=1}^{n}Z_{i}$.
Why should anyone care about this at all?? This is well explained in Abu Mostafa’s book: Learning from data: A Short Course. This is an excellent book for anyone to get started on theoretical machine learning and is a must read. This book was based on the course that he had taught at CalTech. Although this book doesn’t go into the actual proof of Hoeffding’s in-equality this blog aims of explaining Hoeffding’s inequality in a rigorous manner while also providing great intuition. Most of this blog is originally taken, rather blatantly, from Stanford CS229: Supplemental Lecture notes: Hoeffding’s inequality."><link rel=canonical href=https://mandakausthubh.github.io/posts/generalisation-theory/proof-of-hoeffdings-inequality/><link href=/style.min.2d921c18cf1ec555ffc03d59a8adc211c402c68c930c27d6a0c306ab175a8d09.css rel=stylesheet><link href=/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css rel=stylesheet><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/icons/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/icons/favicon-16x16.png><link rel=mask-icon href=/icons/safari-pinned-tab.svg><link rel="shortcut icon" href=/favicon.ico><link rel=manifest href=https://mandakausthubh.github.io/site.webmanifest><meta name=msapplication-config content="/browserconfig.xml"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#434648"><link rel=icon type=image/svg+xml href=/icons/favicon.svg><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g==" crossorigin=anonymous crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity="sha512-LQNxIMR5rXv7o+b1l8+N1EZMfhG7iFZ9HhnbJkTp4zjNr5Wvst75AqUeFDxeRUa7l5vEDyUiAip//r+EFLLCyA==" crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity="sha512-iWiuBS5nt6r60fCz26Nd0Zqe0nbk1ZTIQbl3Kv7kYsX+yKMUFHzjaH2+AnM6vp2Xs+gNmaBAVWJjSmuPw76Efg==" crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})'></script></head><body data-theme=dark class=notransition><script src=/js/theme.min.8961c317c5b88b953fe27525839672c9343f1058ab044696ca225656c8ba2ab0.js integrity="sha256-iWHDF8W4i5U/4nUlg5ZyyTQ/EFirBEaWyiJWVsi6KrA="></script><div class=navbar role=navigation><nav class=menu aria-label="Main Navigation"><a href=https://mandakausthubh.github.io/ class=logo><svg width="25" height="25" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-home"><title/><path d="M3 9l9-7 9 7v11a2 2 0 01-2 2H5a2 2 0 01-2-2z"/><polyline points="9 22 9 12 15 12 15 22"/></svg>
</a><input type=checkbox id=menu-trigger class=menu-trigger>
<label for=menu-trigger><span class=menu-icon><svg width="25" height="25" stroke="currentcolor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7H3.40726"/><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488H3.49301"/><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"/><path stroke-linecap="round" stroke-linejoin="round" d="M.5 12.5V1.5c0-.552285.447715-1 1-1h11C13.0523.5 13.5.947715 13.5 1.5v11C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C.947715 13.5.5 13.0523.5 12.5z"/></svg></span></label><div class=trigger><ul class=trigger-container><li><a class=menu-link href=/>Home</a></li><li><a class="menu-link active" href=/posts/>Posts</a></li><li><a class=menu-link href=/pages/aboutme/>CV</a></li><li class=menu-separator><span>|</span></li></ul><a id=mode href=#><svg class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1"><title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1=".5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1=".5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/></g></svg><svg class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1"><title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1=".5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1=".5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"/><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"/></g></svg></a></div></nav></div><div class="wrapper post"><main class=page-content aria-label=Content><article><header class=header><h1 class=header-title>Hoeffding's Inequality and Generalization</h1><div class=post-meta><time datetime=2025-04-13T03:53:13+05:30 itemprop=datePublished>Apr 13, 2025</time></div></header><details class=toc open><summary><b></b></summary><nav id=TableOfContents><ul><li><a href=#proof-of-hoeffdings-inequality>Proof of Hoeffding&rsquo;s inequality:</a><ul><li><a href=#check-point-1-markovs-inequality>Check Point 1: Markov&rsquo;s inequality</a></li><li><a href=#check-point-2-chebyshevs-inequality>Check Point 2: Chebyshev&rsquo;s Inequality</a></li><li><a href=#check-point-3-moment-generating-functions-and-chernoff-bounds>Check Point 3: Moment Generating Functions and Chernoff Bounds</a></li><li><a href=#final-step-hoeffdings-inequality>Final Step: Hoeffding&rsquo;s Inequality!!</a></li></ul></li></ul></nav></details><div class=page-content><h1 id=what-is-hoeffdings-inequality->What is Hoeffding&rsquo;s Inequality ?</h1><p>Given a sequence of Random Variables that act as I.I.Ds (Identical Independently Distributions) $Z_{1}, Z_{2}, Z_{3} \dots Z_{n}$ with mean $\mu$ and $\mathbf{P}(a \leq Z_{i} \leq b) = 1$ then for any $\epsilon > 0$ we have the following:</p><p>$$
\mathbf{P}(|\bar{Z}_{n} - \mu| > \epsilon) \leq \exp\left( \frac{-2n\epsilon^{2}}{(b-a)^{2}} \right)
$$</p><p>where $\bar{Z_{n}}=\frac{1}{n} \sum_{i=1}^{n}Z_{i}$.</p><p>Why should anyone care about this at all?? This is well explained in Abu Mostafa&rsquo;s book: <strong>Learning from data: A Short Course</strong>. This is an excellent book for anyone to get started on theoretical machine learning and is a must read. This book was based on the course that he had taught at <strong>CalTech</strong>. Although this book doesn&rsquo;t go into the actual proof of Hoeffding&rsquo;s in-equality this blog aims of explaining Hoeffding&rsquo;s inequality in a rigorous manner while also providing great intuition. <em>Most of this blog is originally taken, rather blatantly, from Stanford CS229: Supplemental Lecture notes: Hoeffding&rsquo;s inequality.</em></p><p>One can interpret this as follows: the probability that the mean of the independent samples one collects from a distribution deviates from the mean of the original distribution with at tolerance $\epsilon$ is exponentially decreasing with the number of samples one collects $n$. This is very important thing for folk in machine learning and data science. This is one of the founding basis for the belief that: <em>More data gives better results</em>. We will see in the next few blogs how this plays into the idea of generalisation in Machine Learning Theory.</p><h2 id=proof-of-hoeffdings-inequality>Proof of Hoeffding&rsquo;s inequality:</h2><p>In order to prove Hoeffding&rsquo;s inequality we prove some other smaller theorems who consequence in what we require:</p><h3 id=check-point-1-markovs-inequality>Check Point 1: Markov&rsquo;s inequality</h3><p>Let $Z \geq 0$ be a non-negative random variable. Then $\forall\ t>0$thisEtsOEryR r</p><p>$$
\mathbf{P}(Z \geq t) \leq \frac{\mathbf{E}[Z]}{t}
$$</p><p><strong>Proof:</strong> This is simple and probably familiar to most of you, but let&rsquo;s go over this anyway:</p><p>$$
\begin{aligned}
\mathbf{P} (Z \geq t) & = \int_{t}^{\infty} p(x)dx \\
 & \leq \int_{t}^{\infty} p(x)dx \leq \int_{t}^{\infty} \frac{x}{t} p(x)dx \\ \\
& \leq \frac{1}{t} \int_{t}^{\infty} x p(x)dx \\ \\
& \leq \frac{1}{t}\int_{0}^{\infty} xp(x)dx = \frac{\mathbf{E}[Z]}{t}\\
\end{aligned}
$$</p><p>Hence we have Markov&rsquo;s: $\mathbf{P}(Z \geq t) \leq \frac{\mathbf{E}[Z]}{t}$</p><h3 id=check-point-2-chebyshevs-inequality>Check Point 2: Chebyshev&rsquo;s Inequality</h3><p>Let $Z$ be a random variable with $\mathbf{Var}(Z)$ being finite. The for $t > 0$ we have:</p><p>$$
\mathbf{P}(|Z - \mathbf{E}[Z]| \geq \epsilon) < \frac{\mathbf{Var}(Z)}{\epsilon^{2}}
$$</p><p>The proof for this is a consequence of the previous Markov Inequality:</p><p>$$
\begin{aligned}
\mathbf{P}(|Z - \mathbf{E}(Z)| \geq \epsilon) & = \mathbf{P} ((Z - \mathbf{E}[Z])^{2} \geq \epsilon^{2}) \\
& \leq \frac{\mathbf{E} [(Z-E[Z])^{2}]}{\epsilon^{2}} = \frac{\mathbf{Var(Z)}}{\epsilon^{2}}
\end{aligned}
$$</p><p>Hence we have the following: $\mathbf{P}(|Z - \mathbf{E}[Z]| \geq \epsilon) &lt; \frac{\mathbf{Var}(Z)}{\epsilon^{2}}$.
Some further observations from this is: When ever we have $Z_{1}, Z_{2}, Z_{3}\dots Z_{n}$ are I.I.Ds with $\mathbf{E}[Z_{i}]=0$ we have the following:</p><p>$$
\mathbf{Var} (\bar{Z}) = \frac{1}{n}\sum_{i,j\leq n}\mathbf{E}[Z_{i}Z_{j}]
$$</p><p>Proof of this is:</p><p>$$
\begin{aligned}
\sum_{i,j \leq n} \mathbf{E}[Z_{i} Z_{j}] &= \sum_{i=1}^{n} \mathbf{E}[Z_{i}^{2}] + \sum_{i \neq j} \mathbf{E}[Z_{i} Z_{j}] \\
& =  \sum_{i=1}^{n} \mathbf{E}[Z_{i}^{2}] + \sum_{i \neq j} \mathbf{E}[Z_{i}].\mathbf{E}[Z_{j}] = \sum_{i=1}^{n} \mathbf{E}[Z_{i}^{2}] + \sum_{i \neq j} (\mathbf{E}[Z_{i}])^{2} \\
& =n\mathbf{E}[Z_{i}^{2}] + n(n-1)\mathbf{E}[Z]^{2}
\\
& = n.\mathbf{E}(Z_{i}^{2})
\end{aligned}
$$</p><p>This gives us:</p><p>$$
\mathbf{E} (Z_{i}^{2}) = \mathbf{Var}(Z_{i}) = \frac{1}{n} \sum_{i,j} \mathbf{E}[Z_{i} Z_{j}]
$$</p><p>Now using Chebyshev&rsquo;s inequality on this result we get:</p><p>$$
\mathbf{P}(|\bar{Z}| \geq \epsilon) \leq \frac{\mathbf{Var}(Z_{i})}{\epsilon^{2}} = \frac{1}{n\epsilon^{2}} \sum_{i, j} \mathbf{E}[Z_{i}Z_{j}]
$$</p><p>We will come back to these results back again.</p><h3 id=check-point-3-moment-generating-functions-and-chernoff-bounds>Check Point 3: Moment Generating Functions and Chernoff Bounds</h3><p>Given a random variable $Z$, the corresponding M.G.F is:</p><p>$$
M_{Z}(\lambda) = \mathbf{E}[\exp(\lambda Z)]
$$</p><p>The <strong>Chernoff bound</strong> is a statistical bound stated as follows, given a random variable we have:</p><p>$$
\mathbf{P}(|Z - \mathbf{E}[Z]| \geq \epsilon) \leq \min_{\lambda\geq {0}} \mathbf{E}[\exp(\ \lambda\ |Z - \mathbf{E}[Z]|)] e^{-\lambda\epsilon} = \min_{{\lambda\geq 0}} M_{|Z - \mathbf{E}[Z]|}(\lambda) e^{-\lambda t}
$$</p><p>Proof for the same is, using Markov&rsquo;s inequality:</p><p>$$
\begin{aligned}
\mathbf{P} (|Z - \mathbf{E}[Z]| \geq \epsilon) & = \mathbf{P} (\exp(\lambda|Z-\mathbf{E}[Z]|) \geq e^{\lambda \epsilon}) \\
& \leq \mathbf{E}[\exp(\lambda|Z- \mathbf{E}[Z]|)].e^{-\lambda\epsilon}
\end{aligned}
$$</p><p>Since this is true for all $\lambda > 0$, we can replace this with a minima over varying positive $\lambda&rsquo;s$.</p><p>$$
\mathbf{P}(|Z - \mathbf{E}[Z]| \geq \epsilon) \leq \min_{\lambda} = \min_{{\lambda\geq 0}} \mathbf{E}[\exp(\lambda|Z- \mathbf{E}[Z]|)].e^{-\lambda\epsilon} = \min_{\lambda > 0} M_{|Z - \mathbf{E}[Z]|}(\lambda) e^{-\lambda t}
$$</p><p>This is the chernoff bound. The reason we care about this is that it plays well with summations over Independent distributions, which is a consequence of moment generating functions. We get the following results:</p><p>$$
M_{Z_{1} + Z_{2} +\dots +Z_{n}}(\lambda) = \prod_{i=1}^{n} M_{Z_{i}}(\lambda)
$$</p><h3 id=final-step-hoeffdings-inequality>Final Step: Hoeffding&rsquo;s Inequality!!</h3><p>The true form of Hoeffding&rsquo;s Inequality is not as we mentioned!! The original statement actually only speaks about the <em>Independent Variables</em> but not <em>I.I.Ds</em>, this is because <em>I.I.D</em>s are just are just one version of <em>Independent Variables</em>. The true statement is:</p><p><strong>Hoeffding&rsquo;s Inequality</strong>: Let $Z_{1}, Z_{2} \dots Z_{n}$ are independent variables all bounded in the range $[a, b]$, i.e. $Z_{i} \in [a,b]$ for all $i$. Then we have, $\forall \ \epsilon > 0$:</p><p>$$
\mathbf{P} \left( \left|\frac{1}{n}\sum_{i=1}^{n} (Z_{i} - \mathbf{E}[Z_{i}])\right| \geq \epsilon \right) \leq \exp\left( \frac{-2n\epsilon^{2}}{(b-a)^{2}} \right)
$$</p><p>Before we prove this theorem we prove a different version of this: the <strong>Hoeffding&rsquo;s Lemma</strong>:
Let $Z$ be a bounded random variable with $Z \in [a,b]$. Then:</p><p>$$
\mathbf{E}[\exp(\lambda(Z - \mathbf{E}[Z]))] \leq \exp\left( \frac{\lambda^{2} (b-a)^{2}}{8} \right)
$$</p><p>In this article we stick to a much simpler version of this which is has a factor of 2 instead of 8. We also use the famouse <em>Jensen&rsquo;s Inequality</em>: which states that given a convex function (think of something like $f(x) = x^{2}$, always have a <em>cup shape</em>) $f$, we can always state that $f(\mathbf{E}[x]) \leq \mathbf{E}[f(x)]$.</p></div></article></main></div><footer class=footer><span class=footer_item></span>&nbsp;<div class=footer_social-icons><a href=https://github.com/MandaKausthubh target=_blank rel="noopener noreferrer me" title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=https://x.com/MandaKausthubh target=_blank rel="noopener noreferrer me" title=Twitter><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9.0 01-3.14 1.53 4.48 4.48.0 00-7.86 3v1A10.66 10.66.0 013 4s-4 9 5 13a11.64 11.64.0 01-7 2c9 5 20 0 20-11.5a4.5 4.5.0 00-.08-.83A7.72 7.72.0 0023 3z"/></svg>
</a><a href=https://www.linkedin.com/in/kausthubh-manda/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg></a></div><small class=footer_copyright>© 2025 Kausthubh Manda.</small></footer><a href=# title id=totop><svg width="48" height="48" fill="currentcolor" stroke="currentcolor" viewBox="0 96 960 960"><path d="M283 704.739 234.261 656 480 410.261 725.739 656 677 704.739l-197-197-197 197z"/></svg>
</a><script src=https://mandakausthubh.github.io/js/main.min.35f435a5d8eac613c52daa28d8af544a4512337d3e95236e4a4978417b8dcb2f.js integrity="sha256-NfQ1pdjqxhPFLaoo2K9USkUSM30+lSNuSkl4QXuNyy8="></script></body></html>