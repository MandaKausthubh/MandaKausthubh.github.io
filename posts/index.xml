<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Manda Kausthubh's blog</title><link>https://mandakausthubh.github.io/posts/</link><description>Recent content in Posts on Manda Kausthubh's blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 14 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://mandakausthubh.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Proof of Hoeffding Inequality and Generalization</title><link>https://mandakausthubh.github.io/posts/generalisation-theory/proof-of-hoeffdings-inequality/</link><pubDate>Mon, 14 Apr 2025 00:00:00 +0000</pubDate><guid>https://mandakausthubh.github.io/posts/generalisation-theory/proof-of-hoeffdings-inequality/</guid><description>&lt;h1 id="what-is-hoeffdings-inequality-">What is Hoeffding&amp;rsquo;s Inequality ?&lt;/h1>
&lt;p>Given a sequence of Random Variables that act as I.I.Ds (Identical Independently Distributions) $Z_{1}, Z_{2}, Z_{3} \dots Z_{n}$ with mean $\mu$ and $\mathbf{P}(a \leq Z_{i} \leq b) = 1$ then for any $\epsilon &amp;gt; 0$ we have the following:&lt;/p>
&lt;p>
$$
\mathbf{P}(|\bar{Z}_{n} > \epsilon |) \leq 2 \exp\left( \frac{-2n\epsilon^{2}}{(b-a)^{2}} \right)
$$
&lt;/p>
&lt;p>where $\bar{Z}&lt;em>{n}=\frac{1}{N} \sum&lt;/em>{i=1}^{N}Z_{i}$.&lt;/p>
&lt;p>Why should anyone care about this at all?? This is well explained in Abu Mostafa&amp;rsquo;s book: &lt;strong>Learning from data: A Short Course&lt;/strong>. This is an excellent book for anyone to get started on theoretical machine learning and is a must read. This book was based on the course that he had taught at &lt;strong>CalTech&lt;/strong>.&lt;/p></description></item></channel></rss>